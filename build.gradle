plugins {
  id 'java'
  id 'net.saliman.properties' version '1.5.2'
  id 'com.github.johnrengelman.shadow' version '7.1.2'
  id "com.github.jk1.dependency-license-report" version "1.3"

  // Only used for testing
  id 'com.marklogic.ml-gradle' version '4.3.7'
}

java {
  sourceCompatibility = 1.8
  targetCompatibility = 1.8
}

repositories {
  mavenCentral()

  // For testing
  mavenLocal()
}

configurations {
  documentation
  assets
}

ext {
  kafkaVersion = "3.2.3"
}

dependencies {
  compileOnly "org.apache.kafka:connect-api:${kafkaVersion}"
  compileOnly "org.apache.kafka:connect-json:${kafkaVersion}"
  compileOnly "org.apache.kafka:connect-runtime:${kafkaVersion}"
  compileOnly "org.slf4j:slf4j-api:1.7.36"

  // The latest jackson libraries can be removed once the marklogic-client is upgraded to include them.
  implementation 'com.fasterxml.jackson.core:jackson-core:2.13.4'
  implementation 'com.fasterxml.jackson.core:jackson-annotations:2.13.4'
  implementation 'com.fasterxml.jackson.core:jackson-databind:2.13.4.2'
  implementation 'com.fasterxml.jackson.dataformat:jackson-dataformat-csv:2.13.4'

  implementation "com.marklogic:marklogic-client-api:5.5.4"

  implementation("com.marklogic:marklogic-data-hub:5.7.2") {
    // Excluding these because there's no need for them
    exclude module: "spring-boot-autoconfigure"
    exclude module: "spring-integration-http"
    exclude module: "jaeger-core"
    exclude module: "jaeger-thrift"

    // Excluding because it causes Kafka Connect to complain mightily if included
    exclude module: "logback-classic"
  }

  // 1.2.1 will be published soon, and then we'll switch to that
  testImplementation 'com.marklogic:marklogic-junit5:1.2.1'

  testImplementation "org.apache.kafka:connect-api:${kafkaVersion}"
  testImplementation "org.apache.kafka:connect-json:${kafkaVersion}"
  testImplementation 'net.mguenther.kafka:kafka-junit:3.2.2'

  // Forcing logback to be used for test logging
  testImplementation "ch.qos.logback:logback-classic:1.2.11"
  testImplementation "org.slf4j:jcl-over-slf4j:1.7.36"

  documentation files('LICENSE.txt')
  documentation files('NOTICE.txt')
  documentation files('README.md')

  assets files('MarkLogic_logo.png')
  assets files('apache_logo.png')
}

// This ensures that the compiler reports "unchecked" warnings.
// This helps us use the compiler to prevent potential problems.
tasks.withType(JavaCompile) {
  options.compilerArgs << '-Xlint:unchecked'
  options.deprecation = true
}

test {
  useJUnitPlatform()
}

shadowJar {
  // Exclude DHF source files
  exclude "hub-internal-artifacts/**"
  exclude "hub-internal-config/**"
  exclude "ml-config/**"
  exclude "ml-modules*/**"
  exclude "scaffolding/**"
}

task copyJarToKafka(type: Copy, dependsOn: shadowJar) {
  description = "Used for local development and testing; copies the jar to your local Kafka install"
  from "build/libs"
  into "${kafkaHome}/libs"
}

task copyPropertyFilesToKafka(type: Copy) {
  description = "Used for local development and testing; copies the properties files to your local Kafka install"
  from "config"
  into "${kafkaHome}/config"
  filter { String line ->
    line.startsWith('ml.connection.username=') ? 'ml.connection.username=' + kafkaMlUsername : line
  }
  filter { String line ->
    line.startsWith('ml.connection.password=') ? 'ml.connection.password=' + kafkaMlPassword : line
  }
}

task deploy {
  description = "Used for local development and testing; builds the jar and copies it and the properties files to your local Kafka install"
  dependsOn = ["copyJarToKafka", "copyPropertyFilesToKafka"]
}

ext {
  confluentArchiveGroup = "Confluent Connector Archive"
  confluentTestingGroup = "Confluent Platform Local Testing"
  baseArchiveBuildDir = "build/connectorArchive"
  baseArchiveName = "${componentOwner}-${componentName}-${version}"
}

// Tasks for building the archive required for submitting to the Confluence Connector Hub

import org.apache.tools.ant.filters.ReplaceTokens

task connectorArchive_CopyManifestToBuildDirectory(type: Copy, group: confluentArchiveGroup) {
  description = "Copy the project manifest into the root folder"
  from '.'
  include 'manifest.json'
  into "${baseArchiveBuildDir}/${baseArchiveName}"
  filter(ReplaceTokens, tokens: [CONFLUENT_USER: componentOwner, VERSION: version])
}

task connectorArchive_CopyAssetsToBuildDirectory(type: Copy, group: confluentArchiveGroup) {
  description = "Copy the project assets into the assets folder"
  from configurations.assets
  into "${baseArchiveBuildDir}/${baseArchiveName}/assets"
}

task connectorArchive_CopyEtcToBuildDirectory(type: Copy, group: confluentArchiveGroup) {
  description = "Copy the project support files into the etc folder"
  from 'config'
  include '*'
  into "${baseArchiveBuildDir}/${baseArchiveName}/etc"
}

task connectorArchive_CopyDocumentationToBuildDirectory(type: Copy, group: confluentArchiveGroup) {
  description = "Copy the project documentation into the doc folder"
  from configurations.documentation
  into "${baseArchiveBuildDir}/${baseArchiveName}/doc"
}

task connectorArchive_CopyDependenciesToBuildDirectory(type: Copy, group: confluentArchiveGroup, dependsOn: jar) {
  description = "Copy the dependency jars into the lib folder"
  from jar
  from configurations.runtimeClasspath.findAll { it.name.endsWith('jar') }
  into "${baseArchiveBuildDir}/${baseArchiveName}/lib"
}

task connectorArchive_BuildDirectory(group: confluentArchiveGroup) {
  description = "Build the directory that will be used to create the Kafka Connector Archive"
  dependsOn = [
    connectorArchive_CopyManifestToBuildDirectory,
    connectorArchive_CopyDependenciesToBuildDirectory,
    connectorArchive_CopyDocumentationToBuildDirectory,
    connectorArchive_CopyEtcToBuildDirectory,
    connectorArchive_CopyAssetsToBuildDirectory
  ]
}

task connectorArchive(type: Zip, dependsOn: connectorArchive_BuildDirectory, group: confluentArchiveGroup) {
  description = 'Build a Connector Hub for the Confluent Connector Hub'
  from "${baseArchiveBuildDir}"
  include '**/*'
  archiveName "${baseArchiveName}.zip"
  destinationDir(file('build/distro'))
}

task copyConnectorToConfluent(type: Copy, group: confluentTestingGroup, dependsOn: connectorArchive_BuildDirectory) {
  description = "Build the connector archive and copy it to your local Confluent Platform"
  from baseArchiveBuildDir
  into "${confluentHome}/share/confluent-hub-components"
}

// See https://docs.confluent.io/confluent-cli/current/command-reference/local/confluent_local_destroy.html
task destroyLocalConfluent(type: Exec, group: confluentTestingGroup) {
  description = "Destroy the local Confluent Platform instance"
  commandLine "confluent", "local", "destroy"
  // Main reason this will fail is because Confluent is not running, which shouldn't cause a failure
  ignoreExitValue = true
}

// See https://docs.confluent.io/confluent-cli/current/command-reference/local/services/confluent_local_services_start.html
task startLocalConfluent(type: Exec, group: confluentTestingGroup) {
  description = "Convenience task for starting a local instance of Confluent Platform"
  commandLine "confluent", "local", "services", "start"
}

task loadDatagenPurchasesConnector(type: Exec, group: confluentTestingGroup) {
  description = "Load an instance of the Datagen connector into Confluent Platform for sending JSON documents to " +
    "the 'purchases' topic"
  commandLine "confluent", "local", "services", "connect", "connector", "load", "datagen-purchases-source", "-c",
    "src/test/resources/confluent/datagen-purchases-source.json"
}

task loadMarkLogicPurchasesSinkConnector(type: Exec, group: confluentTestingGroup) {
  description = "Load an instance of the MarkLogic Kafka connector into Confluent Platform for writing data to " +
    "MarkLogic from the 'purchases' topic"
  commandLine "confluent", "local", "services", "connect", "connector", "load", "marklogic-purchases-sink", "-c",
    "src/test/resources/confluent/marklogic-purchases-sink.json"
}

task loadMarkLogicPurchasesSourceConnector(type: Exec, group: confluentTestingGroup) {
  description = "Load an instance of the MarkLogic Kafka connector into Confluent Platform for reading rows from " +
    "the demo/purchases view"
  commandLine "confluent", "local", "services", "connect", "connector", "load", "marklogic-purchases-source", "-c",
    "src/test/resources/confluent/marklogic-purchases-source.json"
}

task setupLocalConfluent(group: confluentTestingGroup) {
  description = "Start a local Confluent Platform instance and load the Datagen and MarkLogic connectors"
}
setupLocalConfluent.dependsOn startLocalConfluent, loadDatagenPurchasesConnector, loadMarkLogicPurchasesSinkConnector, loadMarkLogicPurchasesSourceConnector
loadDatagenPurchasesConnector.mustRunAfter startLocalConfluent
loadMarkLogicPurchasesSinkConnector.mustRunAfter startLocalConfluent
loadMarkLogicPurchasesSourceConnector.mustRunAfter startLocalConfluent
