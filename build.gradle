plugins {
  id 'java'
  id 'net.saliman.properties' version '1.5.2'
  id 'com.github.johnrengelman.shadow' version '7.1.2'
  id "com.github.jk1.dependency-license-report" version "1.19"

  // Only used for testing
  id 'com.marklogic.ml-gradle' version '4.5.0'
  id 'jacoco'
  id "org.sonarqube" version "3.5.0.2730"

  // Used to generate Avro classes. This will write classes to build/generated-test-avro-java and also add that folder
  // as a source root. Since this is commented out by default, the generated Avro test class has been added to
  // src/test/java. This only needs to be uncommented when there's a need to regenerate that class, at which point it
  // should be copied over to src/test/java and then this plugin should be commented out again.
  // id "com.github.davidmc24.gradle.plugin.avro" version "1.6.0"
}

java {
  sourceCompatibility = 1.8
  targetCompatibility = 1.8
}

repositories {
  mavenCentral()

  // For testing
  mavenLocal()
  maven {
    url "https://nexus.marklogic.com/repository/maven-snapshots/"
  }
}

// Do not cache changing modules
configurations.all {
  resolutionStrategy {
    cacheChangingModulesFor 0, 'seconds'
  }
}

configurations {
  documentation
  assets
}

ext {
  kafkaVersion = "3.2.3"
}

dependencies {
  compileOnly "org.apache.kafka:connect-api:${kafkaVersion}"
  compileOnly "org.apache.kafka:connect-json:${kafkaVersion}"
  compileOnly "org.apache.kafka:connect-runtime:${kafkaVersion}"
  compileOnly "org.slf4j:slf4j-api:1.7.36"

  implementation 'com.marklogic:ml-javaclient-util:4.5.0'
  // Force DHF to use the latest version of ml-app-deployer, which minimizes security vulnerabilities
  implementation "com.marklogic:ml-app-deployer:4.5.0"

  implementation "com.fasterxml.jackson.dataformat:jackson-dataformat-csv:2.14.1"

  // Note that in general, the version of the DHF jar must match that of the deployed DHF instance. Different versions
  // may work together, but that behavior is not guaranteed.
  implementation("com.marklogic:marklogic-data-hub:5.8.0") {
    exclude module: "marklogic-client-api"
    exclude module: "ml-javaclient-util"
    exclude module: "ml-app-deployer"

    // No need for mlcp-util, it's only used in 'legacy' DHF 4 jobs
    exclude module: "mlcp-util"
    // Excluding because it causes Kafka Connect to complain mightily if included
    exclude module: "logback-classic"
  }

  testImplementation 'com.marklogic:marklogic-junit5:1.3.0'

  testImplementation "org.apache.kafka:connect-api:${kafkaVersion}"
  testImplementation "org.apache.kafka:connect-json:${kafkaVersion}"
  testImplementation 'net.mguenther.kafka:kafka-junit:3.2.2'

  testImplementation "org.apache.avro:avro-compiler:1.11.1"

  // Forcing logback to be used for test logging
  testImplementation "ch.qos.logback:logback-classic:1.3.5"
  testImplementation "org.slf4j:jcl-over-slf4j:1.7.36"

  documentation files('LICENSE.txt')
  documentation files('NOTICE.txt')
  documentation files('README.md')

  assets files('MarkLogic_logo.png')
  assets files('apache_logo.png')
}

// This ensures that the compiler reports "unchecked" warnings.
// This helps us use the compiler to prevent potential problems.
tasks.withType(JavaCompile) {
  options.compilerArgs << '-Xlint:unchecked'
  options.deprecation = true
}

test {
  useJUnitPlatform()
}

// Configures jacoco test coverage to be included when "test" is run
test {
  finalizedBy jacocoTestReport
}
jacocoTestReport {
  dependsOn test
}
// Enabling the XML report allows for sonar to grab coverage data from jacoco
jacocoTestReport {
  reports {
    xml.enabled true
  }
}


shadowJar {
  // Exclude DHF source files
  exclude "hub-internal-artifacts/**"
  exclude "hub-internal-config/**"
  exclude "ml-config/**"
  exclude "ml-modules*/**"
  exclude "scaffolding/**"
}

task copyJarToKafka(type: Copy, dependsOn: shadowJar) {
  description = "Used for local development and testing; copies the jar to your local Kafka install"
  from "build/libs"
  into "${kafkaHome}/libs"
}

task copyPropertyFilesToKafka(type: Copy) {
  description = "Used for local development and testing; copies the properties files to your local Kafka install"
  from "config"
  into "${kafkaHome}/config"
  filter { String line ->
    line.startsWith('ml.connection.username=') ? 'ml.connection.username=' + kafkaMlUsername : line
  }
  filter { String line ->
    line.startsWith('ml.connection.password=') ? 'ml.connection.password=' + kafkaMlPassword : line
  }
}

task deploy {
  description = "Used for local development and testing; builds the jar and copies it and the properties files to your local Kafka install"
  dependsOn = ["copyJarToKafka", "copyPropertyFilesToKafka"]
}

ext {
  confluentArchiveGroup = "Confluent Connector Archive"
  confluentTestingGroup = "Confluent Platform Local Testing"
  baseArchiveBuildDir = "build/connectorArchive"
  baseArchiveName = "${componentOwner}-${componentName}-${version}"
}

// Tasks for building the archive required for submitting to the Confluence Connector Hub

import org.apache.tools.ant.filters.ReplaceTokens

task connectorArchive_CopyManifestToBuildDirectory(type: Copy, group: confluentArchiveGroup) {
  description = "Copy the project manifest into the root folder"
  from '.'
  include 'manifest.json'
  into "${baseArchiveBuildDir}/${baseArchiveName}"
  filter(ReplaceTokens, tokens: [CONFLUENT_USER: componentOwner, VERSION: version])
}

task connectorArchive_CopyAssetsToBuildDirectory(type: Copy, group: confluentArchiveGroup) {
  description = "Copy the project assets into the assets folder"
  from configurations.assets
  into "${baseArchiveBuildDir}/${baseArchiveName}/assets"
}

task connectorArchive_CopyEtcToBuildDirectory(type: Copy, group: confluentArchiveGroup) {
  description = "Copy the project support files into the etc folder"
  from 'config'
  include '*'
  into "${baseArchiveBuildDir}/${baseArchiveName}/etc"
}

task connectorArchive_CopyDocumentationToBuildDirectory(type: Copy, group: confluentArchiveGroup) {
  description = "Copy the project documentation into the doc folder"
  from configurations.documentation
  into "${baseArchiveBuildDir}/${baseArchiveName}/doc"
}

task connectorArchive_CopyDependenciesToBuildDirectory(type: Copy, group: confluentArchiveGroup, dependsOn: jar) {
  description = "Copy the dependency jars into the lib folder"
  from jar
  // Confluent already includes the Jackson dependencies that this connector depends on. If the connector includes any
  // itself, and the DHF integration is used with the sink connector, then the following error will occur when DHF
  // tries to connect to the Manage API of MarkLogic:
  // java.lang.ClassCastException: com.fasterxml.jackson.datatype.jdk8.Jdk8Module cannot be cast to com.fasterxml.jackson.databind.Module
  //	at org.springframework.http.converter.json.Jackson2ObjectMapperBuilder.registerWellKnownModulesIfAvailable(Jackson2ObjectMapperBuilder.java:849)
  // stackoverflow indicates this may be due to multiple copies of Jackson being on the classpath, as Jdk8Module
  // otherwise should be castable to Module.
  // Testing has verified that excluding all "jackson-" jars still results in the connector working properly with
  // Confluent 7.3.1. This has no impact on using the connector with plain Apache Kafka which does not rely on
  // constructing this connector archive.
  from configurations.runtimeClasspath.findAll { it.name.endsWith('jar') && !it.name.startsWith("jackson-")}
  into "${baseArchiveBuildDir}/${baseArchiveName}/lib"
}

task connectorArchive_BuildDirectory(group: confluentArchiveGroup) {
  description = "Build the directory that will be used to create the Kafka Connector Archive"
  dependsOn = [
    connectorArchive_CopyManifestToBuildDirectory,
    connectorArchive_CopyDependenciesToBuildDirectory,
    connectorArchive_CopyDocumentationToBuildDirectory,
    connectorArchive_CopyEtcToBuildDirectory,
    connectorArchive_CopyAssetsToBuildDirectory
  ]
}

task connectorArchive(type: Zip, dependsOn: connectorArchive_BuildDirectory, group: confluentArchiveGroup) {
  description = 'Build a Connector Hub for the Confluent Connector Hub'
  from "${baseArchiveBuildDir}"
  include '**/*'
  archiveName "${baseArchiveName}.zip"
  destinationDir(file('build/distro'))
}

task deleteConnectorInConfluent(type: Delete) {
  description = "Must delete the existing connector to ensure no old dependencies are left in the connector folder"
  delete "${confluentHome}/share/confluent-hub-components/marklogic-kafka-marklogic-connector-${version}"
}

task copyConnectorToConfluent(type: Copy, group: confluentTestingGroup, dependsOn: [connectorArchive_BuildDirectory, deleteConnectorInConfluent]) {
  description = "Build the connector archive and copy it to your local Confluent Platform"
  from baseArchiveBuildDir
  into "${confluentHome}/share/confluent-hub-components"
}

// See https://docs.confluent.io/confluent-cli/current/command-reference/local/confluent_local_destroy.html
task destroyLocalConfluent(type: Exec, group: confluentTestingGroup) {
  description = "Destroy the local Confluent Platform instance"
  commandLine "confluent", "local", "destroy"
  // Main reason this will fail is because Confluent is not running, which shouldn't cause a failure
  ignoreExitValue = true
}

// See https://docs.confluent.io/confluent-cli/current/command-reference/local/services/confluent_local_services_start.html
task startLocalConfluent(type: Exec, group: confluentTestingGroup) {
  description = "Convenience task for starting a local instance of Confluent Platform"
  commandLine "confluent", "local", "services", "start"
}

task loadDatagenPurchasesConnector(type: Exec, group: confluentTestingGroup) {
  description = "Load an instance of the Datagen connector into Confluent Platform for sending JSON documents to " +
    "the 'purchases' topic"
  commandLine "confluent", "local", "services", "connect", "connector", "load", "datagen-purchases-source", "-c",
    "src/test/resources/confluent/datagen-purchases-source.json"
}

task loadMarkLogicPurchasesSinkConnector(type: Exec, group: confluentTestingGroup) {
  description = "Load an instance of the MarkLogic Kafka connector into Confluent Platform for writing data to " +
    "MarkLogic from the 'purchases' topic"
  commandLine "confluent", "local", "services", "connect", "connector", "load", "marklogic-purchases-sink", "-c",
    "src/test/resources/confluent/marklogic-purchases-sink.json"
}

task loadMarkLogicPurchasesSourceConnector(type: Exec, group: confluentTestingGroup) {
  description = "Load an instance of the MarkLogic Kafka connector into Confluent Platform for reading rows from " +
    "the demo/purchases view"
  commandLine "confluent", "local", "services", "connect", "connector", "load", "marklogic-purchases-source", "-c",
    "src/test/resources/confluent/marklogic-purchases-source.json"
}

task loadMarkLogicAuthorsSourceConnector(type: Exec, group: confluentTestingGroup) {
  description = "Loads a source connector that retrieves authors from the citations.xml file, which is also used for " +
    "all the automated tests"
  commandLine "confluent", "local", "services", "connect", "connector", "load", "marklogic-authors-source", "-c",
    "src/test/resources/confluent/marklogic-authors-source.json"
}

task loadMarkLogicEmployeesSourceConnector(type: Exec, group: confluentTestingGroup) {
  commandLine "confluent", "local", "services", "connect", "connector", "load", "marklogic-employees-source", "-c",
    "src/test/resources/confluent/marklogic-employees-source.json"
}

task setupLocalConfluent(group: confluentTestingGroup) {
  description = "Start a local Confluent Platform instance and load the Datagen and MarkLogic connectors"
}

// Temporarily only loading the source connector to make manual testing easier, will re-enable all of these before 1.8.0
//setupLocalConfluent.dependsOn startLocalConfluent, loadDatagenPurchasesConnector, loadMarkLogicPurchasesSinkConnector, loadMarkLogicPurchasesSourceConnector
setupLocalConfluent.dependsOn startLocalConfluent, loadMarkLogicEmployeesSourceConnector

loadDatagenPurchasesConnector.mustRunAfter startLocalConfluent
loadMarkLogicPurchasesSinkConnector.mustRunAfter startLocalConfluent
loadMarkLogicPurchasesSourceConnector.mustRunAfter startLocalConfluent
loadMarkLogicAuthorsSourceConnector.mustRunAfter startLocalConfluent
loadMarkLogicEmployeesSourceConnector.mustRunAfter startLocalConfluent

task insertAuthors(type: Test) {
  useJUnitPlatform()
  systemProperty "AUTHOR_IDS", authorIds
  description = "Insert a new author into the kafka-test-content database via a new citations XML document; " +
    "use e.g. -PauthorIds=7,8,9 to insert 3 new authors with IDs of 7, 8, and 9"
  include "com/marklogic/kafka/connect/source/debug/InsertAuthorsTest.class"
}
