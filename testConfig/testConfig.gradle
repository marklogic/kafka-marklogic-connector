buildscript {
    repositories {
        jcenter()
    }
    dependencies {
        classpath 'org.hidetake:gradle-ssh-plugin:2.10.1'
    }
}

plugins {
    id 'net.saliman.properties' version '1.5.1'
    id 'org.hidetake.ssh' version '2.10.1'
}

apply plugin: 'org.hidetake.ssh'

def tmpConfigsDir = "tmpConfigsDir"
def projectProps = new Properties()
file("../gradle.properties").withInputStream { projectProps.load(it) }
def version = projectProps.getProperty("version")

// Build the list of various servers
for (server in remoteKafkaBrokers.split(',')) {
    remotes.create(server) {
        role 'kafkaBroker'
        host = server
        user = 'bitnami'
        identity = file('kafka.pem')
    }
}
for (server in remoteConnectorServers.split(',')) {
    remotes.create(server) {
        role 'connectorServer'
        host = server
        user = 'bitnami'
        identity = file('kafka.pem')
    }
}
remotes.create('remoteWorkerServer') {
    role 'remoteWorkerServer'
    host = remoteWorkerServer
    user = 'bitnami'
    identity = file('kafka.pem')
}


task kafkaDeployConnectorRemoteServer {
    doLast {
        ssh.run {
            session(remotes.role('connectorServer')) {
                put from: "build/libs/kafka-connect-marklogic-${version}.jar", into: remoteKafkaLibsDir
            }
        }
    }
}

task copyAndFilterConfigs(type: Copy) {
    outputs.upToDateWhen { false }
    from '../config/marklogic-sink.properties'
    into tmpConfigsDir
    filter { line -> line
            .replaceAll('topics=marklogic', "topics=${kafkaTopics}")
            .replaceAll('ml.connection.host=localhost', "ml.connection.host=${mlHost}")
            .replaceAll('ml.connection.port=8000', "ml.connection.port=${mlPort}")
            .replaceAll('ml.connection.database=Documents', "ml.connection.database=${mlDatabase}")
    }
}

task kafkaDeployConfigsRemoteServer(dependsOn: copyAndFilterConfigs) {
    doLast {
        ssh.run {
            session(remotes.role('connectorServer')) {
                put from: "testConfig/${tmpConfigsDir}/marklogic-sink.properties", into: remoteConnectorConfigDir
            }
        }
    }
}

task kafkaStopRemoteServer {
    doLast {
        ssh.run {
            session(remotes.role('kafkaBroker')) {
                // Execute a command
                def stopResult = execute 'sudo /opt/bitnami/kafka/bin/kafka-server-stop.sh ; sudo rm -rf /opt/bitnami/kafka/tmp/kafka-logs/* ; sudo rm -rf /opt/bitnami/kafka/logs/*'

                // Also Groovy methods or properties are available in a session closure
                println stopResult
            }
        }
    }
}

task kafkaStartRemoteServer {
    doLast {
        ssh.run {
            session(remotes.role('kafkaBroker')) {
                // Execute a command
                def startResult = execute 'sudo /opt/bitnami/kafka/bin/kafka-server-start.sh -daemon /opt/bitnami/kafka/config/server.properties'

                // Also Groovy methods or properties are available in a session closure
                println startResult
            }
        }
    }
}

task kafkaShowLast200LinesRemoteServerLogs {
    doLast {
        ssh.run {
            session(remotes.role('kafkaBroker')) {
                def logResult = execute 'tail -200 /opt/bitnami/kafka/logs/server.log'
                println logResult
            }
        }
    }
}

task kafkaRunMessageProducer {
    doLast {
        ssh.run {
            session(remotes.remoteWorkerServer) {
                def logResult = execute "java -jar /home/bitnami/kafka-producer-1.0-SNAPSHOT.jar -t ${producerTopic} -m ${producerMessagePerThread} -c ${producerThreadCount} -h ${producerZookeeperHost}:${producerZookeeperPort} -n ${producerNullMessages}"
                println logResult
            }
        }
    }
}
